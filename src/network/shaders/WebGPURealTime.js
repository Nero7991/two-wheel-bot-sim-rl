/**
 * Real-time WebGPU Neural Network Inference System
 * 
 * Optimized for real-time robot control applications with sub-5ms latency requirements.
 * Features adaptive optimization, predictive caching, and graceful degradation.
 */

import { WebGPUNeuralNetwork } from './WebGPUNeuralNetwork.js';
import { WebGPUConstants } from './WebGPUConstants.js';

/**
 * Real-time inference optimization modes
 */
export const RealTimeMode = {
    ULTRA_LOW_LATENCY: 'ultra_low_latency',    // <1ms target, minimal validation
    LOW_LATENCY: 'low_latency',                // <3ms target, essential validation
    BALANCED: 'balanced',                      // <5ms target, full validation
    HIGH_THROUGHPUT: 'high_throughput'        // Optimized for batch processing
};

/**
 * Real-time performance targets
 */
export const PerformanceTargets = {
    [RealTimeMode.ULTRA_LOW_LATENCY]: { latency: 1.0, jitter: 0.2, reliability: 0.95 },
    [RealTimeMode.LOW_LATENCY]: { latency: 3.0, jitter: 0.5, reliability: 0.98 },
    [RealTimeMode.BALANCED]: { latency: 5.0, jitter: 1.0, reliability: 0.99 },
    [RealTimeMode.HIGH_THROUGHPUT]: { latency: 10.0, jitter: 2.0, reliability: 0.999 }
};

/**
 * Real-time WebGPU neural network with adaptive optimization
 */
export class WebGPURealTimeNetwork {
    constructor(device, options = {}) {
        this.device = device;
        this.baseNetwork = null;
        
        // Real-time configuration
        this.mode = options.mode || RealTimeMode.BALANCED;
        this.targets = PerformanceTargets[this.mode];
        this.adaptiveOptimization = options.adaptiveOptimization !== false;
        this.enablePredictive = options.enablePredictive !== false;
        
        // Performance monitoring
        this.latencyHistory = [];
        this.jitterHistory = [];
        this.errorCount = 0;
        this.totalInferences = 0;
        this.lastInferenceTime = 0;
        
        // Optimization state
        this.currentOptimizations = new Set();\n        this.optimizationLevel = 0;\n        this.lastOptimizationCheck = 0;\n        this.optimizationInterval = 1000; // Check every 1000 inferences
        
        // Predictive caching
        this.predictionCache = new Map();
        this.cacheHits = 0;\n        this.cacheMisses = 0;\n        this.maxCacheSize = options.maxCacheSize || 100;
        
        // Buffer pre-allocation for zero-copy operations
        this.preallocatedBuffers = new Map();
        this.bufferPool = [];\n        this.bufferPoolSize = options.bufferPoolSize || 10;
        
        // Emergency fallback
        this.fallbackMode = false;\n        this.cpuFallback = null;\n        this.fallbackThreshold = options.fallbackThreshold || 10; // ms
        
        // Robot control specific optimizations
        this.controlFrequency = options.controlFrequency || 200; // Hz
        this.expectedInputRange = options.expectedInputRange || { angle: [-0.2, 0.2], velocity: [-0.1, 0.1] };
        this.smoothingWindow = options.smoothingWindow || 3;
        this.outputHistory = [];
        
        console.log(`Real-time WebGPU network initialized in ${this.mode} mode`);
    }

    /**
     * Initialize real-time network with optimizations
     * @param {number} inputSize - Input layer size
     * @param {number} hiddenSize - Hidden layer size  
     * @param {number} outputSize - Output layer size
     * @param {Object} options - Initialization options
     * @returns {Promise<void>}
     */
    async initialize(inputSize, hiddenSize, outputSize, options = {}) {
        try {
            console.log('Initializing real-time WebGPU network...');
            
            // Create base network
            this.baseNetwork = new WebGPUNeuralNetwork(this.device);
            await this.baseNetwork.initialize(inputSize, hiddenSize, outputSize, {
                ...options,
                realTimeOptimized: true
            });
            
            // Pre-allocate buffers for real-time operation
            await this._preallocateBuffers(inputSize, outputSize);
            
            // Apply initial optimizations based on mode
            await this._applyInitialOptimizations();
            
            // Initialize predictive caching if enabled
            if (this.enablePredictive) {
                this._initializePredictiveCache();
            }
            
            // Run performance calibration
            await this._performanceCalibration();
            
            console.log('Real-time network initialization complete');
            console.log('Performance targets:', this.targets);
            
        } catch (error) {
            console.error('Real-time network initialization failed:', error);
            throw error;
        }
    }\n\n    /**\n     * Real-time inference with adaptive optimization\n     * @param {Float32Array} input - Input state vector\n     * @param {Object} options - Inference options\n     * @returns {Promise<Float32Array>} Output action probabilities\n     */\n    async infer(input, options = {}) {\n        const startTime = performance.now();\n        this.totalInferences++;\n        \n        try {\n            // Input validation (mode-dependent)\n            if (this.mode !== RealTimeMode.ULTRA_LOW_LATENCY) {\n                this._validateInput(input);\n            }\n            \n            // Check predictive cache\n            if (this.enablePredictive) {\n                const cachedResult = this._checkPredictiveCache(input);\n                if (cachedResult) {\n                    this.cacheHits++;\n                    this._recordLatency(performance.now() - startTime);\n                    return this._postProcessOutput(cachedResult, input);\n                }\n                this.cacheMisses++;\n            }\n            \n            // Get pre-allocated buffer\n            const buffer = this._getPreallocatedBuffer();\n            \n            // Real-time inference based on current optimization level\n            let output;\n            if (this.fallbackMode) {\n                output = await this._inferCPUFallback(input);\n            } else {\n                switch (this.optimizationLevel) {\n                    case 0: // Standard inference\n                        output = await this.baseNetwork.forward(input);\n                        break;\n                    case 1: // Real-time optimized\n                        output = await this.baseNetwork.forwardRealTime(input, {\n                            realTime: true,\n                            skipValidation: true,\n                            usePreallocated: true\n                        });\n                        break;\n                    case 2: // Ultra-optimized with pre-allocated buffers\n                        output = await this._ultraOptimizedInference(input, buffer);\n                        break;\n                    default:\n                        output = await this.baseNetwork.forward(input);\n                }\n            }\n            \n            // Post-process output\n            const finalOutput = this._postProcessOutput(output, input);\n            \n            // Update predictive cache\n            if (this.enablePredictive) {\n                this._updatePredictiveCache(input, finalOutput);\n            }\n            \n            // Record performance and check for optimization needs\n            const latency = performance.now() - startTime;\n            this._recordLatency(latency);\n            \n            if (this.adaptiveOptimization) {\n                await this._checkAdaptiveOptimization(latency);\n            }\n            \n            return finalOutput;\n            \n        } catch (error) {\n            this.errorCount++;\n            console.error('Real-time inference failed:', error);\n            \n            // Emergency fallback\n            return this._emergencyFallback(input);\n        }\n    }\n\n    /**\n     * Ultra-optimized inference using pre-allocated buffers\n     * @private\n     */\n    async _ultraOptimizedInference(input, buffer) {\n        // Direct GPU memory operations with minimal overhead\n        const device = this.device;\n        const network = this.baseNetwork;\n        \n        // Upload input directly to pre-allocated buffer\n        device.queue.writeBuffer(buffer.input, 0, input);\n        \n        // Execute optimized compute pipeline\n        const encoder = device.createCommandEncoder({ label: 'ultra_realtime' });\n        const pass = encoder.beginComputePass({ label: 'ultra_compute' });\n        \n        // Hardcoded pipeline for maximum performance\n        pass.setPipeline(network.shaderManager.getPipeline('matmul_simple'));\n        pass.setBindGroup(0, buffer.bindGroups.hidden);\n        pass.dispatchWorkgroups(Math.ceil(network.hiddenSize / 64));\n        \n        pass.setPipeline(network.shaderManager.getPipeline('relu'));\n        pass.setBindGroup(0, buffer.bindGroups.activation);\n        pass.dispatchWorkgroups(Math.ceil(network.hiddenSize / 64));\n        \n        pass.setPipeline(network.shaderManager.getPipeline('matmul_simple'));\n        pass.setBindGroup(0, buffer.bindGroups.output);\n        pass.dispatchWorkgroups(Math.ceil(network.outputSize / 64));\n        \n        pass.end();\n        device.queue.submit([encoder.finish()]);\n        \n        // Read result from pre-allocated staging buffer\n        await device.queue.onSubmittedWorkDone();\n        const MapMode = WebGPUConstants.MapMode;
        await buffer.staging.mapAsync(MapMode.READ);\n        const result = new Float32Array(buffer.staging.getMappedRange());\n        buffer.staging.unmap();\n        \n        return result.slice(); // Copy to avoid buffer reuse issues\n    }\n\n    /**\n     * Pre-allocate buffers for zero-allocation inference\n     * @private\n     */\n    async _preallocateBuffers(inputSize, outputSize) {\n        console.log('Pre-allocating buffers for real-time operation...');\n        \n        for (let i = 0; i < this.bufferPoolSize; i++) {\n            const bufferSet = {\n                input: this.device.createBuffer({\n                    size: inputSize * 4,\n                    usage: WebGPUConstants.BufferUsage.STORAGE | WebGPUConstants.BufferUsage.COPY_DST\n                }),\n                staging: this.device.createBuffer({\n                    size: outputSize * 4,\n                    usage: WebGPUConstants.BufferUsage.MAP_READ | WebGPUConstants.BufferUsage.COPY_DST\n                }),\n                bindGroups: null, // Will be set up after network initialization\n                inUse: false\n            };\n            \n            this.bufferPool.push(bufferSet);\n        }\n        \n        console.log(`Pre-allocated ${this.bufferPoolSize} buffer sets`);\n    }\n\n    /**\n     * Get a pre-allocated buffer from the pool\n     * @private\n     */\n    _getPreallocatedBuffer() {\n        for (const buffer of this.bufferPool) {\n            if (!buffer.inUse) {\n                buffer.inUse = true;\n                return buffer;\n            }\n        }\n        \n        // If no buffers available, create a temporary one\n        console.warn('Buffer pool exhausted, creating temporary buffer');\n        return null;\n    }\n\n    /**\n     * Return buffer to pool\n     * @private\n     */\n    _returnBuffer(buffer) {\n        if (buffer && buffer.inUse) {\n            buffer.inUse = false;\n        }\n    }\n\n    /**\n     * Apply initial optimizations based on mode\n     * @private\n     */\n    async _applyInitialOptimizations() {\n        switch (this.mode) {\n            case RealTimeMode.ULTRA_LOW_LATENCY:\n                this.optimizationLevel = 2;\n                this.currentOptimizations.add('ultra_mode');\n                this.currentOptimizations.add('skip_validation');\n                this.currentOptimizations.add('prealloc_buffers');\n                break;\n                \n            case RealTimeMode.LOW_LATENCY:\n                this.optimizationLevel = 1;\n                this.currentOptimizations.add('realtime_mode');\n                this.currentOptimizations.add('minimal_validation');\n                break;\n                \n            case RealTimeMode.BALANCED:\n                this.optimizationLevel = 0;\n                break;\n                \n            case RealTimeMode.HIGH_THROUGHPUT:\n                this.currentOptimizations.add('batch_optimize');\n                break;\n        }\n        \n        console.log('Applied optimizations:', Array.from(this.currentOptimizations));\n    }\n\n    /**\n     * Performance calibration to validate real-time targets\n     * @private\n     */\n    async _performanceCalibration() {\n        console.log('Running performance calibration...');\n        \n        const testInput = new Float32Array([0.1, -0.05]);\n        const warmupRuns = 50;\n        const testRuns = 200;\n        \n        // Warmup\n        for (let i = 0; i < warmupRuns; i++) {\n            await this.baseNetwork.forward(testInput);\n        }\n        \n        // Calibration test\n        const times = [];\n        for (let i = 0; i < testRuns; i++) {\n            const start = performance.now();\n            await this.infer(testInput);\n            times.push(performance.now() - start);\n        }\n        \n        const stats = this._calculateStats(times);\n        const meetsTargets = {\n            latency: stats.p95 <= this.targets.latency,\n            jitter: stats.std <= this.targets.jitter,\n            reliability: (1 - this.errorCount / testRuns) >= this.targets.reliability\n        };\n        \n        console.log('Calibration results:', {\n            mean: stats.mean.toFixed(2) + 'ms',\n            p95: stats.p95.toFixed(2) + 'ms',\n            jitter: stats.std.toFixed(2) + 'ms',\n            reliability: ((1 - this.errorCount / testRuns) * 100).toFixed(1) + '%',\n            meetsTargets\n        });\n        \n        if (!meetsTargets.latency) {\n            console.warn(`Latency target not met: ${stats.p95.toFixed(2)}ms > ${this.targets.latency}ms`);\n            if (this.mode !== RealTimeMode.ULTRA_LOW_LATENCY) {\n                console.log('Attempting optimization upgrade...');\n                await this._upgradeOptimizations();\n            }\n        }\n    }\n\n    /**\n     * Check for adaptive optimization needs\n     * @private\n     */\n    async _checkAdaptiveOptimization(currentLatency) {\n        if (this.totalInferences % this.optimizationInterval !== 0) {\n            return;\n        }\n        \n        const recentLatencies = this.latencyHistory.slice(-100);\n        if (recentLatencies.length < 50) return;\n        \n        const avgLatency = recentLatencies.reduce((sum, l) => sum + l, 0) / recentLatencies.length;\n        const p95Latency = recentLatencies.sort((a, b) => a - b)[Math.floor(recentLatencies.length * 0.95)];\n        \n        // Check if we need to optimize\n        if (p95Latency > this.targets.latency * 1.2) {\n            console.log(`Performance degradation detected: ${p95Latency.toFixed(2)}ms`);\n            await this._upgradeOptimizations();\n        }\n        \n        // Check if we can relax optimizations (for better accuracy)\n        if (p95Latency < this.targets.latency * 0.7 && this.optimizationLevel > 0) {\n            console.log('Performance headroom available, relaxing optimizations');\n            await this._downgradeOptimizations();\n        }\n    }\n\n    /**\n     * Upgrade optimizations for better performance\n     * @private\n     */\n    async _upgradeOptimizations() {\n        if (this.optimizationLevel < 2) {\n            this.optimizationLevel++;\n            console.log(`Upgraded to optimization level ${this.optimizationLevel}`);\n            \n            switch (this.optimizationLevel) {\n                case 1:\n                    this.currentOptimizations.add('realtime_mode');\n                    this.currentOptimizations.add('skip_validation');\n                    break;\n                case 2:\n                    this.currentOptimizations.add('ultra_mode');\n                    this.currentOptimizations.add('prealloc_buffers');\n                    break;\n            }\n        } else {\n            console.log('Maximum optimization level reached, enabling fallback mode');\n            this.fallbackMode = true;\n        }\n    }\n\n    /**\n     * Downgrade optimizations for better accuracy\n     * @private\n     */\n    async _downgradeOptimizations() {\n        if (this.optimizationLevel > 0) {\n            this.optimizationLevel--;\n            console.log(`Downgraded to optimization level ${this.optimizationLevel}`);\n            \n            // Remove corresponding optimizations\n            if (this.optimizationLevel < 2) {\n                this.currentOptimizations.delete('ultra_mode');\n                this.currentOptimizations.delete('prealloc_buffers');\n            }\n            if (this.optimizationLevel < 1) {\n                this.currentOptimizations.delete('realtime_mode');\n                this.currentOptimizations.delete('skip_validation');\n            }\n        }\n    }\n\n    /**\n     * Initialize predictive caching system\n     * @private\n     */\n    _initializePredictiveCache() {\n        console.log('Initializing predictive caching system...');\n        this.predictionCache.clear();\n        this.cacheHits = 0;\n        this.cacheMisses = 0;\n    }\n\n    /**\n     * Check predictive cache for similar inputs\n     * @private\n     */\n    _checkPredictiveCache(input) {\n        const key = this._generateCacheKey(input);\n        return this.predictionCache.get(key);\n    }\n\n    /**\n     * Update predictive cache with new result\n     * @private\n     */\n    _updatePredictiveCache(input, output) {\n        if (this.predictionCache.size >= this.maxCacheSize) {\n            // Remove oldest entry\n            const firstKey = this.predictionCache.keys().next().value;\n            this.predictionCache.delete(firstKey);\n        }\n        \n        const key = this._generateCacheKey(input);\n        this.predictionCache.set(key, new Float32Array(output));\n    }\n\n    /**\n     * Generate cache key for input state\n     * @private\n     */\n    _generateCacheKey(input) {\n        // Round to reduce key space\n        const precision = 1000;\n        return input.map(x => Math.round(x * precision)).join(',');\n    }\n\n    /**\n     * Post-process output with smoothing and validation\n     * @private\n     */\n    _postProcessOutput(output, input) {\n        // Apply temporal smoothing for robot control stability\n        if (this.smoothingWindow > 1 && this.outputHistory.length > 0) {\n            const smoothed = new Float32Array(output.length);\n            const window = Math.min(this.smoothingWindow, this.outputHistory.length + 1);\n            const weight = 1.0 / window;\n            \n            for (let i = 0; i < output.length; i++) {\n                smoothed[i] = output[i] * weight;\n                for (let j = 0; j < Math.min(window - 1, this.outputHistory.length); j++) {\n                    smoothed[i] += this.outputHistory[this.outputHistory.length - 1 - j][i] * weight;\n                }\n            }\n            \n            this.outputHistory.push(Array.from(output));\n            if (this.outputHistory.length > this.smoothingWindow) {\n                this.outputHistory.shift();\n            }\n            \n            return smoothed;\n        }\n        \n        this.outputHistory.push(Array.from(output));\n        if (this.outputHistory.length > this.smoothingWindow) {\n            this.outputHistory.shift();\n        }\n        \n        return output;\n    }\n\n    /**\n     * Emergency fallback for critical failures\n     * @private\n     */\n    _emergencyFallback(input) {\n        console.warn('Using emergency fallback');\n        \n        // Simple heuristic control for balancing robot\n        const angle = input[0];\n        const velocity = input[1];\n        \n        // Basic proportional control\n        const action = new Float32Array(3);\n        if (angle > 0.05) {\n            action[0] = 0.8; // Left motor\n            action[1] = 0.1; // Brake\n            action[2] = 0.1; // Right motor\n        } else if (angle < -0.05) {\n            action[0] = 0.1; // Left motor\n            action[1] = 0.1; // Brake\n            action[2] = 0.8; // Right motor\n        } else {\n            action[0] = 0.2; // Left motor\n            action[1] = 0.6; // Brake (maintain balance)\n            action[2] = 0.2; // Right motor\n        }\n        \n        return action;\n    }\n\n    /**\n     * CPU fallback inference\n     * @private\n     */\n    async _inferCPUFallback(input) {\n        if (!this.cpuFallback) {\n            // Create CPU fallback network\n            const { CPUBackend } = await import('../CPUBackend.js');\n            this.cpuFallback = new CPUBackend();\n            \n            const arch = this.baseNetwork.getArchitecture();\n            await this.cpuFallback.createNetwork(\n                arch.inputSize,\n                arch.hiddenSize,\n                arch.outputSize\n            );\n            \n            // Copy weights from GPU network\n            const weights = await this.baseNetwork.getWeights();\n            this.cpuFallback.setWeights({\n                architecture: {\n                    inputSize: arch.inputSize,\n                    hiddenSize: arch.hiddenSize,\n                    outputSize: arch.outputSize,\n                    parameterCount: arch.parameterCount\n                },\n                weightsInputHidden: weights.weightsHidden,\n                biasHidden: weights.biasHidden,\n                weightsHiddenOutput: weights.weightsOutput,\n                biasOutput: weights.biasOutput\n            });\n        }\n        \n        return this.cpuFallback.forward(input);\n    }\n\n    /**\n     * Validate input for real-time constraints\n     * @private\n     */\n    _validateInput(input) {\n        if (!input || input.length !== 2) {\n            throw new Error('Invalid input: expected Float32Array of length 2');\n        }\n        \n        // Check input ranges for robot state\n        const angle = input[0];\n        const velocity = input[1];\n        \n        if (Math.abs(angle) > 1.0 || Math.abs(velocity) > 1.0) {\n            console.warn('Input values outside expected range:', { angle, velocity });\n        }\n    }\n\n    /**\n     * Record latency measurement\n     * @private\n     */\n    _recordLatency(latency) {\n        this.latencyHistory.push(latency);\n        if (this.latencyHistory.length > 1000) {\n            this.latencyHistory.shift();\n        }\n        \n        // Calculate jitter\n        if (this.lastInferenceTime > 0) {\n            const jitter = Math.abs(latency - this.lastInferenceTime);\n            this.jitterHistory.push(jitter);\n            if (this.jitterHistory.length > 1000) {\n                this.jitterHistory.shift();\n            }\n        }\n        \n        this.lastInferenceTime = latency;\n    }\n\n    /**\n     * Get comprehensive real-time performance metrics\n     * @returns {Object} Performance metrics and status\n     */\n    getPerformanceMetrics() {\n        const latencyStats = this._calculateStats(this.latencyHistory);\n        const jitterStats = this._calculateStats(this.jitterHistory);\n        \n        return {\n            realTime: {\n                mode: this.mode,\n                targets: this.targets,\n                optimizationLevel: this.optimizationLevel,\n                activeOptimizations: Array.from(this.currentOptimizations),\n                fallbackMode: this.fallbackMode\n            },\n            performance: {\n                totalInferences: this.totalInferences,\n                errorCount: this.errorCount,\n                errorRate: this.totalInferences > 0 ? (this.errorCount / this.totalInferences * 100).toFixed(2) + '%' : '0%',\n                latency: latencyStats,\n                jitter: jitterStats,\n                reliability: this.totalInferences > 0 ? ((this.totalInferences - this.errorCount) / this.totalInferences * 100).toFixed(2) + '%' : '0%'\n            },\n            caching: this.enablePredictive ? {\n                enabled: true,\n                size: this.predictionCache.size,\n                maxSize: this.maxCacheSize,\n                hits: this.cacheHits,\n                misses: this.cacheMisses,\n                hitRate: (this.cacheHits + this.cacheMisses) > 0 ? \n                    (this.cacheHits / (this.cacheHits + this.cacheMisses) * 100).toFixed(1) + '%' : '0%'\n            } : { enabled: false },\n            buffers: {\n                poolSize: this.bufferPoolSize,\n                available: this.bufferPool.filter(b => !b.inUse).length,\n                utilization: ((this.bufferPoolSize - this.bufferPool.filter(b => !b.inUse).length) / this.bufferPoolSize * 100).toFixed(1) + '%'\n            },\n            meetingTargets: {\n                latency: latencyStats.p95 <= this.targets.latency,\n                jitter: jitterStats.mean <= this.targets.jitter,\n                reliability: (this.totalInferences - this.errorCount) / this.totalInferences >= this.targets.reliability\n            }\n        };\n    }\n\n    /**\n     * Calculate statistical measures\n     * @private\n     */\n    _calculateStats(values) {\n        if (values.length === 0) {\n            return { count: 0, min: 0, max: 0, mean: 0, median: 0, p95: 0, std: 0 };\n        }\n        \n        const sorted = [...values].sort((a, b) => a - b);\n        const n = values.length;\n        const mean = values.reduce((sum, v) => sum + v, 0) / n;\n        const variance = values.reduce((sum, v) => sum + Math.pow(v - mean, 2), 0) / n;\n        \n        return {\n            count: n,\n            min: Math.min(...values),\n            max: Math.max(...values),\n            mean,\n            median: sorted[Math.floor(n / 2)],\n            p95: sorted[Math.floor(n * 0.95)],\n            std: Math.sqrt(variance)\n        };\n    }\n\n    /**\n     * Clean up real-time network resources\n     */\n    destroy() {\n        // Clean up buffer pool\n        for (const buffer of this.bufferPool) {\n            if (buffer.input) buffer.input.destroy();\n            if (buffer.staging) buffer.staging.destroy();\n        }\n        this.bufferPool = [];\n        \n        // Clear caches\n        this.predictionCache.clear();\n        \n        // Destroy networks\n        if (this.baseNetwork) {\n            this.baseNetwork.destroy();\n        }\n        if (this.cpuFallback) {\n            // CPU backend doesn't have destroy method\n            this.cpuFallback = null;\n        }\n        \n        console.log('Real-time WebGPU network destroyed');\n    }\n}\n\n/**\n * Factory function for creating real-time networks\n * @param {GPUDevice} device - WebGPU device\n * @param {RealTimeMode} mode - Real-time mode\n * @param {Object} options - Configuration options\n * @returns {WebGPURealTimeNetwork} Real-time network instance\n */\nexport function createRealTimeNetwork(device, mode = RealTimeMode.BALANCED, options = {}) {\n    return new WebGPURealTimeNetwork(device, { mode, ...options });\n}\n\n/**\n * Quick setup for robot control applications\n * @param {GPUDevice} device - WebGPU device\n * @param {Object} config - Robot-specific configuration\n * @returns {Promise<WebGPURealTimeNetwork>} Initialized real-time network\n */\nexport async function setupRobotControlNetwork(device, config = {}) {\n    const options = {\n        mode: config.mode || RealTimeMode.LOW_LATENCY,\n        controlFrequency: config.controlFrequency || 200,\n        enablePredictive: config.enablePredictive !== false,\n        adaptiveOptimization: config.adaptiveOptimization !== false,\n        smoothingWindow: config.smoothingWindow || 3,\n        bufferPoolSize: config.bufferPoolSize || 10,\n        maxCacheSize: config.maxCacheSize || 50,\n        expectedInputRange: config.expectedInputRange || {\n            angle: [-0.2, 0.2],\n            velocity: [-0.1, 0.1]\n        }\n    };\n    \n    const network = new WebGPURealTimeNetwork(device, options);\n    \n    // Initialize with robot-appropriate architecture\n    await network.initialize(\n        2,  // angle, angular velocity\n        config.hiddenSize || 8,\n        3,  // left motor, brake, right motor\n        {\n            initMethod: 'he',\n            realTimeOptimized: true\n        }\n    );\n    \n    return network;\n}